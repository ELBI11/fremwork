# Transfer Learning for Plant Disease Classification - Complete Implementation
# Author: Bilal El Ibrahimi
# Course: AI & Applications - Deep Learning

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
import timm

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support
import time
import os
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ================================
# DATA PREPARATION
# ================================

class PlantVillageDataset:
    """PlantVillage dataset handler with preprocessing"""
    
    def __init__(self, data_dir, batch_size=32, val_split=0.2, test_split=0.1):
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.val_split = val_split
        self.test_split = test_split
        
        # Data transformations
        self.train_transform = transforms.Compose([
            transforms.Resize(256),
            transforms.RandomCrop(224),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        self.val_test_transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # ViT specific transforms
        self.vit_transform = transforms.Compose([
            transforms.Resize(256),
            transforms.RandomCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.RandAugment(num_ops=2, magnitude=9),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
    
    def get_dataloaders(self, use_vit_transforms=False):
        """Create train, validation, and test dataloaders"""
        
        # Choose transforms
        if use_vit_transforms:
            train_transform = self.vit_transform
        else:
            train_transform = self.train_transform
        
        # Load full dataset
        full_dataset = torchvision.datasets.ImageFolder(
            root=self.data_dir,
            transform=None
        )
        
        # Calculate split sizes
        total_size = len(full_dataset)
        test_size = int(self.test_split * total_size)
        val_size = int(self.val_split * total_size)
        train_size = total_size - val_size - test_size
        
        # Split dataset
        train_dataset, val_dataset, test_dataset = random_split(
            full_dataset, [train_size, val_size, test_size]
        )
        
        # Apply transforms
        train_dataset.dataset.transform = train_transform
        val_dataset.dataset.transform = self.val_test_transform
        test_dataset.dataset.transform = self.val_test_transform
        
        # Create dataloaders
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, 
                                shuffle=True, num_workers=4, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, 
                              shuffle=False, num_workers=4, pin_memory=True)
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, 
                               shuffle=False, num_workers=4, pin_memory=True)
        
        return train_loader, val_loader, test_loader, full_dataset.classes

# ================================
# EXERCISE 1: FROM SCRATCH MODELS
# ================================

class VGG19(nn.Module):
    """VGG19 implementation from scratch"""
    
    def __init__(self, num_classes=26):
        super(VGG19, self).__init__()
        
        self.features = nn.Sequential(
            # Block 1
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 2
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 3
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 4
            nn.Conv2d(256, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
            
            # Block 5
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2)
        )
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512 * 7 * 7, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Linear(4096, num_classes)
        )
        
        self._initialize_weights()
    
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

class BasicBlock(nn.Module):
    """ResNet Basic Block"""
    expansion = 1
    
    def __init__(self, in_planes, planes, stride=1):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, 
                              padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, 
                              padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_planes != self.expansion * planes:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, 
                         stride=stride, bias=False),
                nn.BatchNorm2d(self.expansion * planes)
            )
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class ResNet34(nn.Module):
    """ResNet34 implementation from scratch"""
    
    def __init__(self, num_classes=26):
        super(ResNet34, self).__init__()
        self.in_planes = 64
        
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.layer1 = self._make_layer(BasicBlock, 64, 3, stride=1)
        self.layer2 = self._make_layer(BasicBlock, 128, 4, stride=2)
        self.layer3 = self._make_layer(BasicBlock, 256, 6, stride=2)
        self.layer4 = self._make_layer(BasicBlock, 512, 3, stride=2)
        self.linear = nn.Linear(512 * BasicBlock.expansion, num_classes)
        
        self._initialize_weights()
    
    def _make_layer(self, block, planes, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(block(self.in_planes, planes, stride))
            self.in_planes = planes * block.expansion
        return nn.Sequential(*layers)
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        out = self.layer4(out)
        out = F.adaptive_avg_pool2d(out, (1, 1))
        out = out.view(out.size(0), -1)
        out = self.linear(out)
        return out
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

class DenseLayer(nn.Module):
    """DenseNet Dense Layer"""
    
    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):
        super(DenseLayer, self).__init__()
        self.add_module('norm1', nn.BatchNorm2d(num_input_features))
        self.add_module('relu1', nn.ReLU(inplace=True))
        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size * growth_rate,
                                          kernel_size=1, stride=1, bias=False))
        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate))
        self.add_module('relu2', nn.ReLU(inplace=True))
        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,
                                          kernel_size=3, stride=1, padding=1, bias=False))
        self.drop_rate = drop_rate
    
    def forward(self, x):
        new_features = super(DenseLayer, self).forward(x)
        if self.drop_rate > 0:
            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)
        return torch.cat([x, new_features], 1)

class DenseBlock(nn.Module):
    """DenseNet Dense Block"""
    
    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):
        super(DenseBlock, self).__init__()
        for i in range(num_layers):
            layer = DenseLayer(num_input_features + i * growth_rate, growth_rate,
                             bn_size, drop_rate)
            self.add_module('denselayer%d' % (i + 1), layer)

class Transition(nn.Module):
    """DenseNet Transition Layer"""
    
    def __init__(self, num_input_features, num_output_features):
        super(Transition, self).__init__()
        self.add_module('norm', nn.BatchNorm2d(num_input_features))
        self.add_module('relu', nn.ReLU(inplace=True))
        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,
                                         kernel_size=1, stride=1, bias=False))
        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))

class DenseNet121(nn.Module):
    """DenseNet121 implementation from scratch"""
    
    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64,
                 bn_size=4, drop_rate=0, num_classes=26):
        super(DenseNet121, self).__init__()
        
        # First convolution
        self.features = nn.Sequential()
        self.features.add_module('conv0', nn.Conv2d(3, num_init_features, 
                                                   kernel_size=7, stride=2, padding=3, bias=False))
        self.features.add_module('norm0', nn.BatchNorm2d(num_init_features))
        self.features.add_module('relu0', nn.ReLU(inplace=True))
        self.features.add_module('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        
        # Each denseblock
        num_features = num_init_features
        for i, num_layers in enumerate(block_config):
            block = DenseBlock(num_layers=num_layers, num_input_features=num_features,
                             bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)
            self.features.add_module('denseblock%d' % (i + 1), block)
            num_features = num_features + num_layers * growth_rate
            
            if i != len(block_config) - 1:
                trans = Transition(num_input_features=num_features,
                                 num_output_features=num_features // 2)
                self.features.add_module('transition%d' % (i + 1), trans)
                num_features = num_features // 2
        
        # Final batch norm
        self.features.add_module('norm5', nn.BatchNorm2d(num_features))
        self.features.add_module('relu5', nn.ReLU(inplace=True))
        
        # Linear layer
        self.classifier = nn.Linear(num_features, num_classes)
        
        # Official init from torch repo
        self._initialize_weights()
    
    def forward(self, x):
        features = self.features(x)
        out = F.adaptive_avg_pool2d(features, (1, 1))
        out = torch.flatten(out, 1)
        out = self.classifier(out)
        return out
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.constant_(m.bias, 0)

# ================================
# EXERCISE 2: TRANSFER LEARNING
# ================================

def create_transfer_model(architecture, num_classes=26, pretrained=True):
    """Create transfer learning model with pre-trained weights"""
    
    if architecture.lower() == 'vgg19':
        model = models.vgg19(pretrained=pretrained)
        # Freeze feature extraction layers
        for param in model.features.parameters():
            param.requires_grad = False
        # Replace classifier
        model.classifier[6] = nn.Linear(4096, num_classes)
        
    elif architecture.lower() == 'resnet34':
        model = models.resnet34(pretrained=pretrained)
        # Freeze early layers
        for param in model.parameters():
            param.requires_grad = False
        # Unfreeze final layers for fine-tuning
        for param in model.layer4.parameters():
            param.requires_grad = True
        # Replace classifier
        model.fc = nn.Linear(model.fc.in_features, num_classes)
        
    elif architecture.lower() == 'densenet121':
        model = models.densenet121(pretrained=pretrained)
        # Freeze feature layers
        for param in model.features.parameters():
            param.requires_grad = False
        # Replace classifier
        model.classifier = nn.Linear(model.classifier.in_features, num_classes)
    
    else:
        raise ValueError(f"Unknown architecture: {architecture}")
    
    return model

def progressive_unfreeze(model, architecture, stage=1):
    """Progressive unfreezing for advanced fine-tuning"""
    
    if architecture.lower() == 'resnet34':
        if stage >= 1:
            for param in model.fc.parameters():
                param.requires_grad = True
        if stage >= 2:
            for param in model.layer4.parameters():
                param.requires_grad = True
        if stage >= 3:
            for param in model.layer3.parameters():
                param.requires_grad = True
        if stage >= 4:
            for param in model.parameters():
                param.requires_grad = True
                
    elif architecture.lower() == 'vgg19':
        if stage >= 1:
            for param in model.classifier.parameters():
                param.requires_grad = True
        if stage >= 2:
            for param in model.features[-4:].parameters():
                param.requires_grad = True
        if stage >= 3:
            for param in model.features[-8:].parameters():
                param.requires_grad = True
        if stage >= 4:
            for param in model.parameters():
                param.requires_grad = True
                
    elif architecture.lower() == 'densenet121':
        if stage >= 1:
            for param in model.classifier.parameters():
                param.requires_grad = True
        if stage >= 2:
            for param in model.features.denseblock4.parameters():
                param.requires_grad = True
        if stage >= 3:
            for param in model.features.denseblock3.parameters():
                param.requires_grad = True
        if stage >= 4:
            for param in model.parameters():
                param.requires_grad = True

def setup_layerwise_optimizer(model, architecture, base_lr=1e-4, classifier_lr=1e-3):
    """Setup optimizer with different learning rates for different layers"""
    
    params = []
    
    if architecture.lower() == 'vgg19':
        params.append({'params': model.features.parameters(), 'lr': base_lr})
        params.append({'params': model.classifier.parameters(), 'lr': classifier_lr})
    elif architecture.lower() == 'resnet34':
        backbone_params = []
        for name, param in model.named_parameters():
            if 'fc' not in name:
                backbone_params.append(param)
        params.append({'params': backbone_params, 'lr': base_lr})
        params.append({'params': model.fc.parameters(), 'lr': classifier_lr})
    elif architecture.lower() == 'densenet121':
        params.append({'params': model.features.parameters(), 'lr': base_lr})
        params.append({'params': model.classifier.parameters(), 'lr': classifier_lr})
    
    optimizer = optim.Adam(params, weight_decay=1e-4)
    return optimizer

# ================================
# EXERCISE 3: MODERN ARCHITECTURES
# ================================

def create_efficientnet_model(variant='efficientnet_b0', num_classes=26, pretrained=True):
    """Create EfficientNet model using timm library"""
    model = timm.create_model(variant, pretrained=pretrained, num_classes=num_classes)
    
    if pretrained:
        # Progressive unfreezing strategy
        for param in model.parameters():
            param.requires_grad = False
        
        # Unfreeze classifier
        for param in model.classifier.parameters():
            param.requires_grad = True
        
        # Unfreeze final blocks
        if hasattr(model, 'blocks'):
            for param in model.blocks[-2:].parameters():
                param.requires_grad = True
    
    return model

def create_vit_model(variant='vit_base_patch16_224', num_classes=26, pretrained=True):
    """Create Vision Transformer model using timm library"""
    model = timm.create_model(variant, pretrained=pretrained, num_classes=num_classes)
    
    if pretrained:
        # ViT-specific fine-tuning strategy
        for param in model.parameters():
            param.requires_grad = False
        
        # Unfreeze classification head
        for param in model.head.parameters():
            param.requires_grad = True
        
        # Unfreeze final transformer blocks
        if hasattr(model, 'blocks'):
            for param in model.blocks[-3:].parameters():
                param.requires_grad = True
        
        # Keep patch embedding trainable
        if hasattr(model, 'patch_embed'):
            for param in model.patch_embed.parameters():
                param.requires_grad = True
        
        # Keep positional encoding trainable
        if hasattr(model, 'pos_embed'):
            model.pos_embed.requires_grad = True
    
    return model

# ================================
# TRAINING FUNCTIONS
# ================================

def train_epoch(model, train_loader, criterion, optimizer, device):
    """Train for one epoch"""
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc='Training')
    for batch_idx, (data, target) in enumerate(pbar):
        data, target = data.to(device), target.to(device)
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        _, predicted = torch.max(output.data, 1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()
        
        # Update progress bar
        pbar.set_postfix({
            'Loss': f'{running_loss/(batch_idx+1):.3f}',
            'Acc': f'{100.*correct/total:.2f}%'
        })
    
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total
    
    return epoch_loss, epoch_acc

def validate_epoch(model, val_loader, criterion, device):
    """Validate for one epoch"""
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in tqdm(val_loader, desc='Validation'):
            data, target = data.to(device), target.to(device)
            output = model(data)
            val_loss += criterion(output, target).item()
            
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
    
    val_loss /= len(val_loader)
    val_acc = 100. * correct / total
    
    return val_loss, val_acc

def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001, 
                device='cuda', save_path='best_model.pth', patience=10):
    """Complete training loop with early stopping"""
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)
    
    # Training history
    train_losses, train_accs = [], []
    val_losses, val_accs = [], []
    
    best_val_acc = 0.0
    patience_counter = 0
    
    print(f"Starting training for {num_epochs} epochs...")
    print(f"Using device: {device}")
    
    start_time = time.time()
    
    for epoch in range(num_epochs):
        print(f'\nEpoch {epoch+1}/{num_epochs}')
        print('-' * 60)
        
        # Training phase
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        
        # Validation phase
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)
        val_losses.append(val_loss)
        val_accs.append(val_acc)
        
        # Learning rate scheduling
        scheduler.step(val_loss)
        
        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        print(f'Learning Rate: {optimizer.param_groups[0]["lr"]:.6f}')
        
        # Early stopping and model saving
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_val_acc': best_val_acc,
                'train_losses': train_losses,
                'train_accs': train_accs,
                'val_losses': val_losses,
                'val_accs': val_accs
            }, save_path)
            patience_counter = 0
        else:
            patience_counter += 1
        
        if patience_counter >= patience:
            print(f"\nEarly stopping after {epoch+1} epochs")
            break
    
    training_time = time.time() - start_time
    print(f"\nTraining completed in {training_time/3600:.2f} hours")
    print(f"Best validation accuracy: {best_val_acc:.2f}%")
    
    return {
        'train_losses': train_losses,
        'train_accs': train_accs,
        'val_losses': val_losses,
        'val_accs': val_accs,
        'best_val_acc': best_val_acc,
        'training_time': training_time
    }

# ================================
# EVALUATION FUNCTIONS
# ================================

def evaluate_model(model, test_loader, classes, device):
    """Comprehensive model evaluation"""
    model.eval()
    
    all_preds = []
    all_targets = []
    all_probs = []
    
    with torch.no_grad():
        for data, target in tqdm(test_loader, desc='Testing'):
            data, target = data.to(device), target.to(device)
            output = model(data)
            probs = F.softmax(output, dim=1)
            
            _, predicted = torch.max(output, 1)
            
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    # Calculate metrics
    accuracy = accuracy_score(all_targets, all_preds)
    precision, recall, f1, support = precision_recall_fscore_support(
        all_targets, all_preds, average='weighted'
    )
    
    # Classification report
    class_report = classification_report(
        all_targets, all_preds, target_names=classes, output_dict=True
    )
    
    # Confusion matrix
    cm = confusion_matrix(all_targets, all_preds)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'classification_report': class_report,
        'confusion_matrix': cm,
        'predictions': all_preds,
        'targets': all_targets,
        'probabilities': all_probs
    }

def count_parameters(model):
    """Count total and trainable parameters"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    return total_params, trainable_params

def calculate_model_size(model):
    """Calculate model size in MB"""
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    
    size_mb = (param_size + buffer_size) / 1024 / 1024
    return size_mb

# ================================
# VISUALIZATION FUNCTIONS
# ================================

def plot_training_history(history, title="Training History"):
    """Plot training and validation curves"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # Loss plot
    ax1.plot(history['train_losses'], label='Train Loss', color='blue')
    ax1.plot(history['val_losses'], label='Validation Loss', color='red')
    ax1.set_title('Model Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # Accuracy plot
    ax2.plot(history['train_accs'], label='Train Accuracy', color='blue')
    ax2.plot(history['val_accs'], label='Validation Accuracy', color='red')
    ax2.set_title('Model Accuracy')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy (%)')
    ax2.legend()
    ax2.grid(True)
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):
    """Plot confusion matrix"""
    plt.figure(figsize=(12, 10))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=classes, yticklabels=classes)
    plt.title(title)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

def compare_models_performance(results_dict):
    """Compare multiple models performance"""
    models = list(results_dict.keys())
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.ravel()
    
    for i, metric in enumerate(metrics):
        values = [results_dict[model][metric] for model in models]
        bars = axes[i].bar(models, values, color=['blue', 'red', 'green', 'orange', 'purple'][:len(models)])
        axes[i].set_title(f'{metric.capitalize()} Comparison')
        axes[i].set_ylabel(metric.capitalize())
        axes[i].set_ylim(0, 1)
        
        # Add value labels on bars
        for bar, value in zip(bars, values):
            axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                        f'{value:.3f}', ha='center', va='bottom')
        
        plt.setp(axes[i].get_xticklabels(), rotation=45, ha='right')
    
    plt.tight_layout()
    plt.show()

# ================================
# MAIN EXECUTION FUNCTIONS
# ================================

def run_experiment(architecture, data_dir, num_classes=26, use_pretrained=True, 
                  num_epochs=50, batch_size=32):
    """Run complete experiment for a given architecture"""
    
    print(f"\n{'='*80}")
    print(f"EXPERIMENT: {architecture.upper()} ({'Pre-trained' if use_pretrained else 'From Scratch'})")
    print(f"{'='*80}")
    
    # Prepare data
    dataset_handler = PlantVillageDataset(data_dir, batch_size=batch_size)
    
    # Use ViT transforms for Vision Transformer
    use_vit_transforms = 'vit' in architecture.lower()
    train_loader, val_loader, test_loader, classes = dataset_handler.get_dataloaders(
        use_vit_transforms=use_vit_transforms
    )
    
    print(f"Dataset loaded: {len(classes)} classes")
    print(f"Train samples: {len(train_loader.dataset)}")
    print(f"Val samples: {len(val_loader.dataset)}")
    print(f"Test samples: {len(test_loader.dataset)}")
    
    # Create model
    if use_pretrained:
        if 'efficientnet' in architecture.lower():
            model = create_efficientnet_model(architecture, num_classes, pretrained=True)
        elif 'vit' in architecture.lower():
            model = create_vit_model(architecture, num_classes, pretrained=True)
        else:
            model = create_transfer_model(architecture, num_classes, pretrained=True)
    else:
        if architecture.lower() == 'vgg19':
            model = VGG19(num_classes)
        elif architecture.lower() == 'resnet34':
            model = ResNet34(num_classes)
        elif architecture.lower() == 'densenet121':
            model = DenseNet121(num_classes=num_classes)
        else:
            raise ValueError(f"From-scratch implementation not available for {architecture}")
    
    model = model.to(device)
    
    # Model info
    total_params, trainable_params = count_parameters(model)
    model_size = calculate_model_size(model)
    
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"Model size: {model_size:.2f} MB")
    
    # Train model
    save_path = f"{architecture.lower()}_{'pretrained' if use_pretrained else 'scratch'}.pth"
    
    history = train_model(
        model, train_loader, val_loader, 
        num_epochs=num_epochs, device=device, save_path=save_path
    )
    
    # Load best model for evaluation
    checkpoint = torch.load(save_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # Evaluate model
    results = evaluate_model(model, test_loader, classes, device)
    
    # Add model info to results
    results['total_params'] = total_params
    results['trainable_params'] = trainable_params
    results['model_size_mb'] = model_size
    results['training_time'] = history['training_time']
    results['history'] = history
    
    # Plot results
    plot_training_history(history, f"{architecture.upper()} Training History")
    
    print(f"\nFinal Results:")
    print(f"Test Accuracy: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
    print(f"Precision: {results['precision']:.4f}")
    print(f"Recall: {results['recall']:.4f}")
    print(f"F1-Score: {results['f1']:.4f}")
    print(f"Training Time: {results['training_time']/3600:.2f} hours")
    
    return results

def main():
    """Main function to run all experiments"""
    
    # Configuration
    DATA_DIR = "./PlantVillage"  # Update with your data path
    NUM_CLASSES = 26  # Update based on your dataset
    NUM_EPOCHS = 30  # Reduced for demo
    BATCH_SIZE = 32
    
    print("Starting Transfer Learning Experiments")
    print(f"Device: {device}")
    
    # Dictionary to store all results
    all_results = {}
    
    # Exercise 1: From Scratch Models
    print("\n" + "="*50)
    print("EXERCISE 1: FROM SCRATCH IMPLEMENTATIONS")
    print("="*50)
    
    architectures_scratch = ['vgg19', 'resnet34', 'densenet121']
    for arch in architectures_scratch:
        try:
            results = run_experiment(
                architecture=arch,
                data_dir=DATA_DIR,
                num_classes=NUM_CLASSES,
                use_pretrained=False,
                num_epochs=NUM_EPOCHS,
                batch_size=BATCH_SIZE
            )
            all_results[f"{arch}_scratch"] = results
        except Exception as e:
            print(f"Error running {arch} from scratch: {e}")
    
    # Exercise 2: Pre-trained Models
    print("\n" + "="*50)
    print("EXERCISE 2: TRANSFER LEARNING WITH PRE-TRAINED MODELS")
    print("="*50)
    
    architectures_pretrained = ['vgg19', 'resnet34', 'densenet121']
    for arch in architectures_pretrained:
        try:
            results = run_experiment(
                architecture=arch,
                data_dir=DATA_DIR,
                num_classes=NUM_CLASSES,
                use_pretrained=True,
                num_epochs=NUM_EPOCHS,
                batch_size=BATCH_SIZE
            )
            all_results[f"{arch}_pretrained"] = results
        except Exception as e:
            print(f"Error running {arch} pretrained: {e}")
    
    # Exercise 3: Modern Architectures
    print("\n" + "="*50)
    print("EXERCISE 3: MODERN ARCHITECTURES")
    print("="*50)
    
    modern_architectures = ['efficientnet_b0', 'vit_base_patch16_224']
    for arch in modern_architectures:
        try:
            results = run_experiment(
                architecture=arch,
                data_dir=DATA_DIR,
                num_classes=NUM_CLASSES,
                use_pretrained=True,
                num_epochs=NUM_EPOCHS,
                batch_size=BATCH_SIZE
            )
            all_results[arch] = results
        except Exception as e:
            print(f"Error running {arch}: {e}")
    
    # Final comparison
    print("\n" + "="*50)
    print("FINAL COMPARISON")
    print("="*50)
    
    if all_results:
        # Create summary table
        print("\nSummary Results:")
        print("-" * 100)
        print(f"{'Model':<25} {'Accuracy':<10} {'F1-Score':<10} {'Params(M)':<12} {'Size(MB)':<10} {'Time(h)':<8}")
        print("-" * 100)
        
        for model_name, results in all_results.items():
            print(f"{model_name:<25} {results['accuracy']:<10.3f} {results['f1']:<10.3f} "
                  f"{results['total_params']/1e6:<12.2f} {results['model_size_mb']:<10.1f} "
                  f"{results['training_time']/3600:<8.2f}")
        
        # Plot comparison
        compare_models_performance(all_results)
        
        # Save results
        import pickle
        with open('transfer_learning_results.pkl', 'wb') as f:
            pickle.dump(all_results, f)
        print("\nResults saved to 'transfer_learning_results.pkl'")
    
    print("\n" + "="*50)
    print("EXPERIMENTS COMPLETED")
    print("="*50)

if __name__ == "__main__":
    main()
